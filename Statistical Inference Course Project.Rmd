---
title: "Statistical Inference Course Project"
author: "robertwcw"
date: "12/13/2020"
output: 
  html_document: 
    fig_caption: yes
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
# Sys.setenv(TZ = "UTC")      # set global TZ to UTC for POSIXt class

.Rfliburl <- "https://raw.githubusercontent.com/robertwcw/Rflib/master"
source(file.path(.Rfliburl,"getRflib.R"),local = TRUE)
source(getRflib("is.defined.R"),local = TRUE)
source(getRflib("myplclust.R"),local = TRUE)
source(getRflib("strCap.R"),local = TRUE)
```

```{r init.local}
if (!requireNamespace("ggplot2",quietly = TRUE)) install.packages("ggplot2")
library(ggplot2)

par.def <- par(no.readonly = TRUE) 
```
&nbsp;

### Synopsis

This course work embarks on the objective to help us understand how simulation and Central Limit Theorem (CLT) are useful in statistical inference studies (resampling, confidence interval and hypothesis testing etc...) when real-world data isn't readily available or an ideal size of sampling data is constrained by the inevitable factors such as budget, time-limit and manpower etc. 

Simulation is handy for pumping up the sampling data size to enable a more normal distribution approach for statistical analysis, while CLT states that a sampling distribution of the statistic estimates for a variable, of sufficiently large sample size will approximate normal distribution regardless of the variable's distribution in the population, with the sampling estimates converging around the population estimates. 
&nbsp;

### Statistical Simulation

This course work will require two simulated random exponential distribution datasets, namely $X_e$ (X-exponential) and $X_\mu$ (X-mu), to demonstrate how CLT explains the statistics of a sample dataset regardless of its distribution in the population from where the sample was drawn, will eventually fall into a normal distribution pattern when the size of sampling data increased to sufficient number.

Specification of the two datasets used in this demonstration, as follows:  

|   |   |  
|------------------|------------------------------------------------------------|  
| $n = 40$ | Number of random exponential per sample |  
| $B = 1000$ | Number of simulation |  
| $\lambda = \frac {1} {5}$ | Rate value for random exponential function |  
| $\mu = \frac {1} {\lambda}$ | Theoretical mean |  
| $\sigma  = \frac {1} {\lambda}$ | Theoretical standard deviation |  
| $X_e$ | Simulated dataset of 1000 random exponential |  
| $X_\mu$ | Simulated dataset of 1000 sample means of 40 random exponential per sample |
&nbsp;

```{r stat.simul, echo=TRUE}
# statistical simulation

n <- 40                 #number of exponential per sample 
B <- 1000               #number of simulation 
lambda <- 1/5           #rate value for random exponential function
mu <- 1/lambda          #theoretical mean of population
sigma <- 1/lambda       #theoretical std-deviation of population
alpha <- 1/20           #alpha level for confidence interval test

# generate 1000 simulated distribution of 40 random exponential per sample
z <- matrix(nrow = B, ncol = n) 
for (i in 1:B) {z[i,] <- rexp(n, lambda)}; rm(i)

e <- NULL
e <- cbind(rexp(n*25, lambda), # 1000 sample random exponential distribution data
           apply(z, 1, mean))  # 1000 simulated means of each sample of 40 random exponential distributed data
e <- as.data.frame(e)
colnames(e) <- c("X_e", "X_mu")
```
&nbsp;

| Statistics Estimates of $X_\mu$ |
|------------------------|---------------------------------------------------|
| $\bar{X_\mu}$ = `r round(mean(e$X_mu), 4)` | Mean of $X_\mu$ |  
| $\sigma_{X_\mu}$ = `r round(sd(e$X_mu), 4)` | Standard Deviation of $X_\mu$ |  
| $\sigma_{X_\mu}^2$ = `r round(var(e$X_mu), 4)` | Variance of $X_\mu$ |  
| $SE_{X_\mu}$ = `r round(sd(e$X_mu)/sqrt(B), 4)` | Standard Error of $X_\mu$ |
&nbsp;

5-Number Summary of $X_\mu$  
```{r Xmu.summa}
summary(e$X_mu)
```
&nbsp;

```{r plot.Xmu.boxplot}
# boxplot for bootstrapped mean of random exponential sample data 'X_mu'
b <- ggplot()
b <- b + geom_boxplot(data = e, mapping = aes(x = X_mu), 
                      outlier.colour = "darkorange",
                      size = 0.5,
                      varwidth = FALSE)
b <- b + labs(x = expression("X"[mu]), caption = "Figure-1")
print(b)  
```
&nbsp;

The shape formation of boxplot graph for dataset $X_\mu$ in Figure-1 suggests that it is normal distributed. Construct of the boxplot spot the median occurs near the center inside the box with the lower and upper whiskers just about equal length on both sides of the box shall assert our premise about the distribution of $X_\mu$ is normal. Juxtaposing the boxplot in Figure-1 and the histogram in Figure-2 further strengthen our assertion.  
&nbsp;

```{r plot.Xmu.hist}
# histogram for bootstrapped of 1000 mean of random exponential sample data 'X_mu'
h <- ggplot(data = e, mapping = aes(x = X_mu, y = ..density..)) 
h <- h + geom_histogram(color = "black", fill = "white", bins = n) +
        geom_density(alpha = 0.5, fill = "#FF6666", col = "red") +
        geom_vline(aes(xintercept = mean(x = X_mu)), lty = 1, lwd = 1, col = "blue") +
        geom_vline(aes(xintercept = mu), lty = 2, lwd = 0.8, col = "green") +
        labs(x = expression("X"[mu]), caption = "Figure-2")
print(h)
```
&nbsp;

In Figure-2, the histogram graph exhibits a symmetrical (almost) bell-shape density curve overlapping on the histogram suggesting the $X_\mu$ dataset approximates normal distribution. Mean $\bar{X_\mu}$ = `r round(mean(e$X_mu), 4)` is said to be statistically equal to theoretical mean $\mu$, implying that the sample mean is the true mean of the population and this assumption is reinforced by the sample variance $\sigma_{X_\mu}^2 =$ `r round(var(e$X_mu), 4)` and sample standard error $SE_{X_\mu} =$ `r round(sd(e$X_mu)/sqrt(B), 4)` respectively signify the data points of $X_\mu$ will converge around the theoretical mean. 

While both theoretical variance $\sigma^2$ and sample variance $\sigma_{X_\mu}^2$ are used for measuring the average distance the data points from the mean of respective population and sample dataset. Compare $\sigma^2 = 25$ against $\sigma_{X_\mu}^2 =$ `r round(var(e$X_mu), 4)` is quite apparent to our naked eyes that the sample variance is more representative of the variability of the population data than the theoretical variance if the sample mean $\bar{X_\mu}$ is true and equivalent to the theoretical population mean.  
&nbsp;

```{r h.test, echo=TRUE}
# t-test
CI_t <- mean(e$X_mu) + c(-1,1) * qt(1 - alpha/2, NROW(e$X_mu) - 1) * sqrt(var(e$X_mu)/length(e$X_mu))
t_val <- (mean(e$X_mu) - mu)/(sd(e$X_mu)/sqrt(NROW(e$X_mu)))
p_val <- pt(q = abs(t_val), df = B - 1, lower.tail = FALSE) * 2  #one-sided p-value
```
&nbsp;
<!-- # z-test -->
<!-- CI_z <- mean(e$X_mu) + c(-1,1) * qnorm(1 - alpha/2) * sqrt(var(e$X_mu)/B) -->
<!-- z_val <- (mean(e$X_mu) - mu)/(sd(e$X_mu)/sqrt(NROW(e$X_mu))) -->
<!-- p_val <- pnorm(abs(z_val), lower.tail = FALSE) * 2  #p-value for one-sided test -->
<!-- # p_val <- pnorm(abs(z_val), lower.tail = FALSE)      #p-value for two-sided test -->

Let's do a student's T-test to verify our claims; hypothesis test set up as follow:  
**H~0~** : $\bar{X_\mu} = \mu$  
**H~A~** : $\bar{X_\mu} \neq \mu$  
$CI_t = \bar{X_\mu} \pm t_{(1-\frac{\alpha}{2},B-1)}\sqrt\frac{\sigma_\bar{X}^2}{B} =$ `r round(CI_t, 4)`  
$t.value = \frac{\bar{X_\mu}-\mu}{SE_\bar{X_\mu}} =$ `r round(t_val, 4)`  
$p.value = pt(q=abs(t.value),df=B-1,lower.tail=F)*2 =$ `r round(p_val, 4)`  

$p.value =$ `r round(p_val, 4)` $> \alpha$ `r alpha*100`$\%$ level implies the hypothesis test failed to reject true **H~0~** , which is saying the true **H~0~** is accepted for our assumption was true that the sample mean $\bar{X_\mu}$ represents the true mean of the population. Confidence interval $CI_t$ (lower = `r round(CI_t[1], 4)` & upper = `r round(CI_t[2], 4)`) tightly wraps around the theoretical mean $\mu$ indicates the sample estimates will occur very closely to the $\bar{X_\mu}$ between lower bound and upper bound $95\%$ of the time. In other words, the sample mean $\bar{X_\mu}$ is estimating what it was intended to measure. On the same note our claim that the sample variance was true for the population is correct.

### Sample Mean vs Theoretical Mean

Results from statistical simulation tell us that the sample mean $\bar{X_\mu} =$ `r round(mean(e$X_mu), 4)` is estimating the population's theoretical mean $\mu$. 

### Sample Variance vs Theoretical Variance

Sample variance $\sigma_\mu^2 =$ `r round(var(e$X_mu), 4)` is much smaller than the population's theoretical variance $\sigma^2$, hinting the population distribution is not Gaussian as compare to the sample distribution.  
&nbsp;

Note that the vertical blue solid line plotted on the histogram graph in Figure-2 represents the sample mean $\bar{X_\mu}$ which is positioned ever so closely next to the theoretical mean $\mu$, where $t.value$ will tells us if $\bar{X_\mu}$ is less than or greater than $\mu$.  
&nbsp;

### Distributions

Though Central Limit Theorem (CLT) states that the distribution of the sample statistic will become more Gaussian when the number of sampling data is sufficiently large. Let us take the dataset $X_e$ consisting of 1000 random exponential, which we believe is sizeable enough a sample to demonstrate how certain it will look Gaussian according to the CLT definition. 
&nbsp;

| Statistics Estimates of $X_e$ |
|------------------------|---------------------------------------------------|
| $\bar{X_e}$ = `r round(mean(e$X_e), 4)` | Mean of $X_e$ |  
| $\sigma_{X_e}$ = `r round(sd(e$X_e), 4)` | Standard Deviation of $X_e$ |  
| $\sigma_{X_e}^2$ = `r round(var(e$X_e), 4)` | Variance of $X_e$ |  
| $SE_{X_e}$ = `r round(sd(e$X_e)/sqrt(B), 4)` | Standard Error of $X_e$ |  
&nbsp;

5-Number Summary of $X_e$     
```{r Xe.summa}
summary(e$X_e)
```
&nbsp;

```{r plot.Xe.boxplot}
# boxplot for exponential distributed random sample data 'X_e'
b <- ggplot()
b <- b + geom_boxplot(data = e, mapping = aes(x = X_e), 
                      outlier.colour = "darkorange",
                      size = 0.5,
                      varwidth = FALSE)
b <- b + labs(x = expression("X"[e]), caption = "Figure-3")
print(b)
```
&nbsp;

```{r plot.Xe.hist}
# histogram for 1000 random exponential sample data 'X_e'
h <- ggplot(data = e, mapping = aes(x = X_e, y = ..density..)) 
h <- h + 
    geom_histogram(color = "black", fill = "white", bins = n) +
    geom_density(alpha = 0.5, fill = "#FF6666", col = "red") +
    geom_vline(aes(xintercept = mean(x = X_e)), lty = 1, lwd = 1, col = "blue") +
    geom_vline(aes(xintercept = mu), lty = 2, lwd = 0.8, col = "green") +
    labs(x = expression("X"[e]), caption = "Figure-4")
print(h)

```
&nbsp;

Boxplot graph in Figure-3 and histogram (with density curve) graph in Figure-4 are plotted based on the same $X_e$ dataset, clearly they don't look Gaussian at all but are skewed (right skewed in this case). The phenomenon explains that CLT only works on the sampling statistic e.g. sampling means, but not on the as-is sampling data irregardless of size.

Note that mean $\bar{X_e}$ and variane $\sigma_{X_e}^2$ of the dataset $X_e$ are both echoing the theoretical mean $\mu$ and $\sigma$, whereas only the $\bar{X_\mu}$ mimic the theoretical mean but $\sigma_{X_\mu}^2$ does not resonate theoretical variance, which is in line with our prior supposition about the population distribution is not Gaussian compare to the sample distribution.

