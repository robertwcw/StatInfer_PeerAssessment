---
title: "Statistical Inference Course Project"
author: "robertwcw"
date: "12/13/2020"
output: 
  html_document: 
    fig_caption: yes
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
# Sys.setenv(TZ = "UTC")      # set global TZ to UTC for POSIXt class

.Rfliburl <- "https://raw.githubusercontent.com/robertwcw/Rflib/master"
source(file.path(.Rfliburl,"getRflib.R"),local = TRUE)
source(getRflib("is.defined.R"),local = TRUE)
source(getRflib("myplclust.R"),local = TRUE)
source(getRflib("strCap.R"),local = TRUE)
```

```{r init.local}
if (!requireNamespace("ggplot2",quietly = TRUE)) install.packages("ggplot2")
library(ggplot2)

par.def <- par(no.readonly = TRUE) 
```
&nbsp;

### Synopsis

This course work embarks on the objective to help us understand how simulation and Central Limit Theorem (CLT) are useful in statistical inference studies (resampling, confidence interval and hypothesis testing etc...) when the real-world data isn't readily available or ideal size of population data is constrained by the inevitable factors such as budget, time-limit and manpower etc. 

Simulation is handy for pumping up the sampling data size to enable a more normal distribution approach for statistical analysis. While CLT states that a sampling distribution of the statistic estimates for a variable, of sufficiently large sample size will approximate normal distribution regardless of the variable's distribution in the population, with the sampling estimates converging around the population estimates. In other words, when the simulated sampling means of a bootstrapped sample distribution converges around the population mean, we can deduce that the sample distributions are representative of the larger population distribution.
&nbsp;

```{r stat.simul}
# statistical simulation

n <- 40                 #number of exponential per sample 
B <- 1000               #bootstrap size
lambda <- 1/5           #observed rate of population
mu <- 1/lambda          #mean of population
siqma <- 1/lambda       #std-deviation of population
alpha <- 0.05           #alpha level for confidence interval test

# generate 1000 simulated distribution of 40 random exponential per sample
z <- matrix(nrow = B, ncol = n) 
for (i in 1:B) {z[i,] <- rexp(n, lambda)}; rm(i)

e <- NULL
e <- cbind(rexp(n*25, lambda), # 1000 sample random exponential distributed data
           apply(z, 1, mean))  # 1000 simulated means of each sample of 40 random exponential distributed data
e <- as.data.frame(e)
colnames(e) <- c("X_e", "X_mu")
```

### Statistical Simulation

This course work requires some simulated random exponential distribution data to demonstrate how CLT explains a sample dataset (right-skewed in this case) regardless of its distribution in the population from where the sample was drawn, will eventually fall into a normal distribution pattern when the number of sampling data are increased to a sufficient level. 

Specification of the dataset used in this demonstration, as follows:  

| Statistics Estimates | Description |  
|------------------|------------------------------------------------------------|  
| $n = 40$ | Number of random exponential per sample |  
| $B = 1000$ | Number of simulation of sample of 40 random exponential each |  
| $\lambda = \frac {1} {5}$ | Theoretical rate |  
| $\mu = \frac {1} {\lambda}$ | Theoretical mean |  
| $\sigma  = \frac {1} {\lambda}$ | Theoretical standard deviation |  
| $X_e$ | Simulated dataset of 1000 random exponential |  
| $X_\mu$ |Simulated dataset of 1000 sample means of 40 random exponential per sample |
&nbsp;

5-Number Summary of $X_\mu$  
```{r Xmu.summa}
summary(e$X_mu)
```
&nbsp;

| Statistics Estimates of $X_\mu$ |
|------------------------|---------------------------------------------------|
| $\bar{X}$ = `r round(mean(e$X_mu), 5)` | Mean of $X_\mu$ |  
| $\sigma_{X_\mu}$ = `r round(sd(e$X_mu), 5)` | Standard Deviation of $X_\mu$ |  
| $\sigma_{X_\mu}^2$ = `r round(var(e$X_mu), 5)` | Variance of $X_\mu$ |  
| $SE_{X_\mu}$ = `r round(sd(e$X_mu)/sqrt(B), 5)` | Standard Error of $X_\mu$ |
&nbsp;

```{r plot.Xmu.boxplot}
# boxplot for bootstrapped mean of random exponential sample data 'X_mu'
b <- ggplot()
b <- b + geom_boxplot(data = e, mapping = aes(x = X_mu), 
                      outlier.colour = "darkorange",
                      size = 0.5,
                      varwidth = FALSE)
b <- b + labs(x = expression("X"[mu]), caption = "Figure-1")
print(b)  
```
&nbsp;

The boxplot graph in Figure-1 indicates $X_\mu$ dataset is normally distributed, with some outliers lying beyond the extreme ends of either minimum or maximum level, or both. Outlier is almost an inherent characteristic of a random exponential distribution, but not always. The histogram graph in Figure-2 below  reaffirms the postulation illustrated in Figure-1.
&nbsp;

```{r plot.Xmu.hist}
# histogram for bootstrapped of 1000 mean of random exponential sample data 'X_mu'
h <- ggplot(data = e, mapping = aes(x = X_mu, y = ..density..)) 
h <- h + geom_histogram(color = "black", fill = "white", bins = n) +
        geom_density(alpha = 0.3, fill = "#FF6666", col = "red") +
        geom_vline(aes(xintercept = mean(x = X_mu)), lty = 2, lwd = 0.6, col = "blue") +
        labs(x = expression("X"[mu]), caption = "Figure-2")
print(h)

# t-test
CI_t <- mean(e$X_mu) + c(-1,1) * qt(1 - alpha/2, NROW(e$X_mu) - 1) * sqrt(var(e$X_mu)/length(e$X_mu))
t_val <- (mean(e$X_mu) - mu)/(sd(e$X_mu)/sqrt(NROW(e$X_mu)))
p_val <- pt(q = abs(t_val), df = length(e$X_mu) - 1, lower.tail = FALSE) * 2  #p-value for one-sided test

# # z-test
# CI_z <- mean(e$X_mu) + c(-1,1) * qnorm(1 - alpha/2) * (sd(e$X_mu)/sqrt(length(e$X_mu)))
# z_val <- (mean(e$X_mu) - mu)/sqrt(var(e$X_mu)/NROW(e$X_mu))
# p_val <- pnorm(abs(z_val), lower.tail = FALSE) * 2  #p-value for one-sided test
# # p_val <- pnorm(abs(z_val), lower.tail = FALSE)      #p-value for two-sided test

```
&nbsp;

In Figure-2, the histogram graph exhibits a almost symmetrical bell-shape density curve plot suggesting the $X_\mu$ dataset approximates normal distribution. Mean $\bar{X}$ = `r round(mean(e$X_mu), 5)` is statistically equal to the theoretical mean $\mu$, implying that the sample mean is the true mean of the population and this assumption is reinforced by sample variance $\sigma_{X_\mu}^2 =$ `r round(var(e$X_mu), 5)` and sample standard error $SE_{X_\mu} =$ `r round(sd(e$X_mu)/sqrt(B), 5)` respectively signify the sample means of $X_\mu$ converging around the theoretical mean. Whereas the theoretical variance $\sigma^2$ vs sample variance $\sigma_{X_\mu}^2$ of $X_\mu$ explains the random exponential distribution $X_e$ dataset is skewed (not normal). 

Let's do a student's T-test to verify our hypothesis, where:  
**H~0~** : $\bar{X} = \mu$  
**H~A~** : $\bar{X} \neq \mu$  
$CI_t = \bar{X} \pm t_{(1-\frac{\alpha}{2},B-1)}\sqrt\frac{\sigma_\bar{X}^2}{B} =$ `r round(CI_t, 5)`  
$t.value = \frac{\bar{X}-\mu}{SE_\bar{X}} =$ `r round(t_val, 5)`
$p.value = pt(q=abs(t.value),df=B-1,lower.tail=F)*2 =$ `r round(p_val, 5)`

$p.value =$ `r round(p_val, 5)` $> \alpha$ $5\%$ level implies the hypothesis test failed to reject true **H~0~**, which is saying **H~0~** is accepted for our assumption was true that the sample mean $\bar{X}$ represents the true mean of the population. Confidence interval $CI_t$ (lower `r round(CI_t[1], 5)` & upper `r round(CI_t[2], 5)`) are tightly clustering around the true mean $\mu$ indicate that the sample means $\bar{X}$ will occur very closely to the true mean $95\%$ of the time. In other words, the sample mean $\bar{X}$ is estimating what it was intended to measure.

The blue dash vertical line on the histogram graph in Figure-2 represents the sample mean $\bar{X}$ which is positioned ever so closely next to the theoretical mean $\mu$, where $t.value$ tells us if the $\bar{X}$ will fall on the left or right of the $\mu$.  




Summary of $X_e$ simulated random exponential dataset   
```{r Xe.summa}
summary(e$X_e)
```
$X_e$ mean $\mu$ is round(mean(e$X_e), 5) 
$\lambda$ 


```{r plot.hist.Xe}
# plot graph for the simulated data

# histogram for exponential distributed sample data 'X_e'
h <- ggplot(data = e, mapping = aes(x = X_e, y = ..density..)) 
h + geom_histogram(color = "black", fill = "white", binwidth = n/30) + 
    geom_density(alpha = 0.3, fill = "#FF6666", col = "red") +
    geom_vline(aes(xintercept = mean(x = X_e)), lty = 1, lwd = 0.6, col = "blue") +
    geom_vline(aes(xintercept = mu), lty = 2, lwd = 0.6, col = "green") +
    xlab(expression("X"[e]))


# boxplot for exponential distributed random sample data 'X_e'
b <- ggplot()
b <- b + geom_boxplot(data = e, mapping = aes(x = X_e), 
                      outlier.colour = "darkorange",
                      size = 0.5,
                      arwidth = FALSE)
b <- b + labs(x = expression("X"[e]))
print(b)





# t-test
CI_t <- mean(e$X_mu) + c(-1,1) * qt(1-alpha/2,NROW(e$X_mu)-1) * (sd(e$X_mu)/sqrt(length(e$X_mu)))
t_val <- (mean(e$X_mu) - mu)/(sd(e$X_mu)/sqrt(NROW(e$X_mu)))
p_val <- pt(q = abs(t_val), df = length(e$X_mu)-1, lower.tail=FALSE)*2

# z-test
CI_z <- mean(e$X_mu) + c(-1,1)*qnorm(1-alpha/2)*(sd(e$X_mu)/sqrt(length(e$X_mu)))
z_val <- (mean(e$X_mu) - mu)/(sd(e$X_mu)/sqrt(NROW(e$X_mu)))
p_val <- pnorm(abs(z_val), lower.tail = FALSE)*2

```


Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
