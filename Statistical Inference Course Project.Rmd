---
title: "Statistical Inference Course Project"
author: "robertwcw"
date: "12/13/2020"
output: 
  html_document: 
    fig_caption: yes
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
# Sys.setenv(TZ = "UTC")      # set global TZ to UTC for POSIXt class

# .Rfliburl <- "https://raw.githubusercontent.com/robertwcw/Rflib/master"
# source(file.path(.Rfliburl,"getRflib.R"),local = TRUE)
# source(getRflib("is.defined.R"),local = TRUE)
# source(getRflib("myplclust.R"),local = TRUE)
# source(getRflib("strCap.R"),local = TRUE)
```

```{r init.local}
if (!requireNamespace("ggplot2",quietly = TRUE)) install.packages("ggplot2")
library(ggplot2)

if (!requireNamespace("datasets",quietly = TRUE)) install.packages("datasets")
library(datasets)

if (!requireNamespace("modelr",quietly = TRUE)) install.packages("modelr")
library(modelr)

par.def <- par(no.readonly = TRUE) 
```
&nbsp;

# Synopsis

This course work embarks on the objective to help us understand how simulation and Central Limit Theorem (CLT) are useful in statistical inference studies (resampling, confidence interval and hypothesis testing etc...) when real-world data isn't readily available or an ideal size of sampling data is constrained by the inevitable factors such as budget, time-limit and manpower et cetera. Simulation is handy for pumping up the sampling data size to facilitate statistical analysis using normal distribution approach as CLT states that statistics estimates of the sampling distribution size sufficiently large will approximate normal distribution with the sampling estimates converging around the population estimates, irregardless of the population underlying distribution.

First part of the course work is to gauge our understanding on Statistical Simulation and Central Limit Theorem.  

Second part of the course work is to assess our comprehension in basic Statistical Inference using R's "ToothGrowth" dataset package.  
&nbsp;

## Statistical Simulation

This course work will require two simulated random exponential distribution, namely $X_e$ (X-exponential) and $X_\mu$ (X-mu), to demonstrate how CLT explains the statistics of a sample dataset irregardless of its distribution in the population from where the sample was drawn, will eventually fall into normal distribution pattern when the size of sampling data increased to sufficient number.

Specification of the two random exponential datasets used in this demonstration, as follow:  

$Table-1$  

|   |   |  
|------------------|------------------------------------------------------------|  
| $n = 40$ | Number of random exponential per sample |  
| $B = 1000$ | Number of simulation |  
| $\lambda = \frac {1} {5}$ | Rate value for random exponential function |  
| $\mu = \frac {1} {\lambda}$ | Theoretical mean |  
| $\sigma  = \frac {1} {\lambda}$ | Theoretical standard deviation |  
| $X_e$ | Simulated dataset of 1000 random exponential |  
| $X_\mu$ | Simulated dataset of 1000 sample means of 40 random exponential per sample |
&nbsp;

```{r stat.simul, echo=TRUE}
# statistical simulation

n <- 40                 #number of exponential per sample 
B <- 1000               #number of simulation 
lambda <- 1/5           #rate value for random exponential function
mu <- 1/lambda          #theoretical mean of population
sigma <- 1/lambda       #theoretical std-deviation of population
alpha <- 1/20           #alpha level for confidence interval test

# generate 1000 simulated distribution of 40 random exponential per sample
z <- matrix(nrow = B, ncol = n) 
for (i in 1:B) {z[i,] <- rexp(n, lambda)}; rm(i)

e <- NULL
e <- cbind(rexp(n*25, lambda), # 1000 sample random exponential distribution data
           apply(z, 1, mean))  # 1000 simulated means of each sample of 40 random exponential distributed data
e <- as.data.frame(e)
colnames(e) <- c("X_e", "X_mu")
```
&nbsp;

$Table-2$  

| Statistics Estimates of $X_\mu$ |  
|------------------------|---------------------------------------------------|  
| $\bar{X_\mu}$ = `r round(mean(e$X_mu), 4)` | Mean of $X_\mu$ |  
| $\sigma_{X_\mu}$ = `r round(sd(e$X_mu), 4)` | Standard Deviation of $X_\mu$ |  
| $\sigma_{X_\mu}^2$ = `r round(var(e$X_mu), 4)` | Variance of $X_\mu$ |  
| $SE_{X_\mu}$ = `r round(sd(e$X_mu)/sqrt(B), 4)` | Standard Error of $X_\mu$ |  
&nbsp;
5-Number Summary of $X_\mu$  
```{r Xmu.summa}
summary(e$X_mu)
```
&nbsp;

### Exploratory graph plots  
```{r plot.Xmu.boxplot}
# boxplot for bootstrapped mean of random exponential sample data 'X_mu'
b <- ggplot()
b <- b + geom_boxplot(data = e, mapping = aes(x = X_mu, colour = "Bootstrapped"), 
                      outlier.colour = "darkorange",
                      size = 0.5,
                      varwidth = FALSE)
b <- b + labs(x = expression("X"[mu]), caption = "Figure-1")
print(b)  
```
&nbsp;

The shape formation of boxplot graph for $X_\mu$ dataset in Figure-1 looks like that of normal distribution. The construct of the boxplot spot the median occurs near the center inside the box with lower and upper whiskers just about of equal length on both sides of the box shall assert our premise about the distribution of $X_\mu$ is Gaussian. Juxtaposing the boxplot in Figure-1 and the histogram in Figure-2 further strengthen our assertion.  
&nbsp;

```{r plot.Xmu.hist}
# histogram for bootstrapped of 1000 mean of random exponential sample data 'X_mu'
h <- ggplot(data = e, mapping = aes(x = X_mu, y = ..density..)) 
h <- h + geom_histogram(color = "black", fill = "white", bins = n) +
        geom_density(alpha = 0.5, fill = "#FF6666", col = "red") +
        geom_vline(aes(xintercept = mean(x = X_mu), color = "Exponen mean"), lty = 1, lwd = 1) +
        geom_vline(aes(xintercept = mu, color = "Hypothe mean"), lty = 2, lwd = 0.8) +
        labs(x = expression("X"[mu]), caption = "Figure-2")
h <- h + scale_color_manual(name = "statistics", values = c("blue", "green"))
print(h)
```
&nbsp;

The histogram graph in Figure-2 exhibits an overlapping symmetrical (virtually) bell-shape density curve suggests that the $X_\mu$ dataset approximates normal distribution. Mean $\bar{X_\mu}$ = `r round(mean(e$X_mu), 4)` (green vertical line) is said to be statistically equivalent to theoretical mean $\mu$ (blue vertical line) as shown, implying that the sample mean $\bar{X_\mu}$ is very accurately estimating the true mean of the population and this postulation is reinforced by the sample variance $\sigma_{X_\mu}^2 =$ `r round(var(e$X_mu), 4)` and sample standard error $SE_{X_\mu} =$ `r round(sd(e$X_mu)/sqrt(B), 4)` respectively signify the data points of $X_\mu$ will converge around the theoretical mean. Comparing $\sigma^2 =$ `r sigma^2` against $\sigma_{X_\mu}^2 =$ `r round(var(e$X_mu), 4)` also explain the distribution for $X_e$ is far from normal whereas the distribution for $X_\mu$ is approximate normal.  
&nbsp;

### Hypothesis Test

```{r h.test, echo=TRUE}
# t-test

# confidence interval
CI_t <- mean(e$X_mu) + c(-1,1) * qt(1 - alpha/2, NROW(e$X_mu) - 1) * sqrt(var(e$X_mu)/length(e$X_mu))

# t-value
t_val <- (mean(e$X_mu) - mu)/(sd(e$X_mu)/sqrt(NROW(e$X_mu)))

# p-value
# p-value (as-is) is for two-sided test
# p-value (x 2) is for one-sided test
p_val <- pt(q = abs(t_val), df = B - 1, lower.tail = FALSE) * 2  
```
&nbsp;
<!-- # z-test -->
<!-- CI_z <- mean(e$X_mu) + c(-1,1) * qnorm(1 - alpha/2) * sqrt(var(e$X_mu)/B) -->
<!-- z_val <- (mean(e$X_mu) - mu)/(sd(e$X_mu)/sqrt(NROW(e$X_mu))) -->
<!-- p_val <- pnorm(abs(z_val), lower.tail = FALSE) * 2  #p-value for one-sided test -->
<!-- # p_val <- pnorm(abs(z_val), lower.tail = FALSE)      #p-value for two-sided test -->

Let's do a student's T-test to verify our claims; hypothesis test set up as follow:    
**H~0~** : $\bar{X_\mu} = \mu$  

**H~A~** : $\bar{X_\mu} \neq \mu$  

$CI_t = \bar{X_\mu} \pm t_{(1-\frac{\alpha}{2},B-1)}\sqrt\frac{\sigma_\bar{X}^2}{B} =$ `r round(CI_t, 4)`  

$t.value = \frac{\bar{X_\mu}-\mu}{SE_\bar{X_\mu}} =$ `r round(t_val, 4)`  

$p.value = pt(q=abs(t.value),df=B-1,lower.tail=F)*2 =$ `r round(p_val, 4)`  
&nbsp;

$p.value =$ `r round(p_val, 4)` $> \alpha$ `r alpha*100`$\%$ level implies the hypothesis test failed to reject the **H~0~** , which is saying the true **H~0~** is accepted for our hypothesis was true that the sample mean $\bar{X_\mu}$ postulating the true mean of the population. Confidence interval $CI_t$ (`r round(CI_t[1], 4)` ~ `r round(CI_t[2], 4)`) tightly clasp around the theoretical mean $\mu$ indicates the sample estimates will occur very closely to the $\bar{X_\mu}$ between lower bound and upper bound $95\%$ of the time. In other words, the sample mean $\bar{X_\mu}$ is estimating what it was intended to measure. On the same note our claim that the sample variance was true for the population is also true.
&nbsp;

#### Sample Mean vs Theoretical Mean

Results from statistical simulation tell us that the sample mean $\bar{X_\mu} =$ `r round(mean(e$X_mu), 4)` is estimating the population's theoretical mean $\mu$.  
&nbsp;

#### Sample Variance vs Theoretical Variance

Sample variance $\sigma_\mu^2 =$ `r round(var(e$X_mu), 4)` is much smaller than the population's theoretical variance $\sigma^2$, hinting the population distribution is not Gaussian as compare to the sample distribution which is more normal.  
&nbsp;

#### Distributions

Though Central Limit Theorem (CLT) states that the distribution of the sample statistic will become more Gaussian when the number of sampling data is sufficiently large. Let us take the dataset $X_e$ which is comprising 1000 random exponential, which we believe is sizeable enough a sample to demonstrate how certain it will look Gaussian according to the CLT definition.  
&nbsp;

$Table-3$  

| Statistics Estimates of $X_e$ | |  
|------------------------|---------------------------------------------------|  
| $\bar{X_e}$ = `r round(mean(e$X_e), 4)` | Mean of $X_e$ |  
| $\sigma_{X_e}$ = `r round(sd(e$X_e), 4)` | Standard Deviation of $X_e$ |  
| $\sigma_{X_e}^2$ = `r round(var(e$X_e), 4)` | Variance of $X_e$ |  
| $SE_{X_e}$ = `r round(sd(e$X_e)/sqrt(B), 4)` | Standard Error of $X_e$ |  
&nbsp;

5-Number Summary of $X_e$  
```{r Xe.summa}
summary(e$X_e)
```

&nbsp;

```{r plot.Xe.boxplot}
# boxplot for exponential distributed random sample data 'X_e'
b <- ggplot()
b <- b + geom_boxplot(data = e, mapping = aes(x = X_e, colour = "Random exp"), 
                      outlier.colour = "darkorange",
                      size = 0.5,
                      varwidth = FALSE)
b <- b + labs(x = expression("X"[e]), caption = "Figure-3")
print(b)
```

&nbsp;

```{r plot.Xe.hist}
# histogram for 1000 random exponential sample data 'X_e'
h <- ggplot(data = e, mapping = aes(x = X_e, y = ..density..)) 
h <- h + 
    geom_histogram(color = "black", fill = "white", bins = n) +
    geom_density(alpha = 0.5, fill = "#FF6666", col = "red") +
    geom_vline(aes(xintercept = mean(x = X_e), color = "Expon mean"), lty = 1, lwd = 1) +
    geom_vline(aes(xintercept = mu, color = "Hypo mean"), lty = 2, lwd = 0.8) +
    labs(x = expression("X"[e]), caption = "Figure-4")
h <- h + scale_color_manual(name = "statistics", values = c("blue", "green"))
print(h)
```

&nbsp;

Boxplot graph in Figure-3 and histogram (with density curve) graph in Figure-4 are plotted based on the same $X_e$ dataset, clearly they don't look Gaussian at all but are skewed (right skewed in this case). This phenomenon explains that only sampling statistics, e.g. sampling means, will approximate normal distribution for adequately large sample size irregardless of the underlying data distribution. Nevertheless, the as-is data in the $X_e$ dataset will not approach Gaussian distribution irregardless of sample size.  

Note that mean $\bar{X_e}$ and variane $\sigma_{X_e}^2$ of $X_e$ dataset are both echoing the theoretical mean $\mu$ and $\sigma$ correctly described the distribution pattern of $X_e$ is not influenced by sample size.  

**In essence, Central Limit Theorem explains that the statistics of a sufficiently large sampling dataset will approximate normal distribution naturally, irregardless of its underlying distribution.**  
&nbsp;

## Basic Inferential Data Analysis  
&nbsp;

This is the second part of the course work. We will perform basic statistical inferential data analysis on the coefficient between tooth growth and supplement types vs dosage. Include herein the structure of ToothGrowth dataset loaded into R's work space.
```{r load.ToothGrowth}
data("ToothGrowth")
delayedAssign("tg", ToothGrowth)
rm(ToothGrowth)
```
&nbsp;
**Structure of ToothGrowth dataset** 
```{r eda.ToothGrowth.1}
# structure of ToothGrowth dataset
str(tg)
```
&nbsp;
ToothGrowth sampling dataset has 60 rows with 3 variables stored in data frame structure.  
- len ......: Length of tooth {discrete variable}  
- supp ...: supplement type {OJ = 1, VC = 2}  
- dose ...: dosage {$\frac{1}{2}$, 1, 2}  
&nbsp;

### Exploratory data analysis  
&nbsp;

**5-Number summary**  
```{r eda.ToothGrowth.2}
summary(tg)
```
* 60 test subjects split into two groups by supplement type: 30 participants in **OJ** group and 30 participants in **VC** group.  

&nbsp;

#### Theoretical Statistics  
Let's assume the following theoretical statistics for the population data that we are attempting to infer. Theoretical statistics are generally referring to the population statistics that are unknown at this stage, yet we want to ascertain via hypothesis testing.  

$Table-4$  

| Theoretical Statistics for ToothGrowth dataset | |  
|------------------------|---------------------------------------------------|  
| $\mu$ = `r round(mean(tg$len),4)` | Theoretical mean |  
| $\sigma$ = `r round(sd(tg$len),4)` | Theoretical standard deviation |  
| $\sigma^2$ = `r round(var(tg$len),4)` | Theoretical variance |  
&nbsp;

#### Exploratory graph plots  
&nbsp;
Figure-5 and Figure-6 are initial exploratory plots on ToothGrowth sampling data (see Data Structure). The data distribution does not look Gaussian.  

```{r eda.ToothGrowth.3}
b <- ggplot()
b <- b + geom_boxplot(data = tg, mapping = aes(x = len, colour = "Base sample"), 
                      outlier.colour = "darkorange",
                      size = 0.5,
                      varwidth = FALSE)
b <- b + labs(x = "Tooth Length", caption = "Figure-5")
print(b)  
```

&nbsp;

```{r eda.ToothGrowth.4}
h <- ggplot(data = tg, mapping = aes(x = len, y = ..density..)) 
h <- h + geom_histogram(color = "black", fill = "white", bins = 60) +
         geom_density(alpha = 0.5, fill = "#FF6666", col = "red") +
         geom_vline(aes(xintercept = mean(x = len), colour = "Sample mean"), lty = 1, lwd = 1) +
        labs(x = "Tooth Length", caption = "Figure-6")
h <- h + scale_color_manual(name = "statistics", values = "blue")
print(h)
```

&nbsp;

#### Data analysis summary
```{r eda.ToothGrowth.5, warning=FALSE, fig.cap="Figure-7", fig.align="left"}
scatter.smooth(x = tg$dose, y = tg$len,
               ylab = "Tooth Length",
               xlab = "Supplements Dosage",
               main = "Overall Rate of Growth (Type OJ & Type VC)",
               lpars = list(col = "green", lwd = 3, lty = 1))   
```
&nbsp;

There is an overall up trend in the average rate of tooth growth length on the test subjects administered with either **OJ** supplement or **VC** supplement. The test subjects were separated into two groups of 30 participants each by supplement type. In Figure-7, the green smooth curve drawn on the scatter plot exhibits a phenomenon where the test subjects given lower dosage of between 1/2 to 1 dose experienced faster average tooth growth rate than test subjects given higher dosage of 2 doses experienced slower average growth rate irrespective of type of supplement.  

Let's take the scatter plot further by breaking down the supplements administered on the test subjects into two groups by type **OJ** and type **VC**.  
&nbsp;

```{r eda.ToothGrowth.6, warning=FALSE, fig.cap="Figure-8", fig.align="left"}
par(mfrow = c(1,2))

with(subset(tg, as.numeric(supp) == 1), 
     scatter.smooth(x = dose, y = len,
                    main = "OJ Supplement",
                    ylab = "Tooth Length",
                    xlab = "Dosage",
                    lpars = list(col = "red", lwd = 3, lty = 1))   
     )
abline(v = mean(subset(tg, as.numeric(supp) == 1)$dose), col = "yellow1")

with(subset(tg, as.numeric(supp) == 2), 
     scatter.smooth(x = dose, y = len,
                    main = "VC Supplement",
                    ylab = "Tooth Length",
                    xlab = "Dosage",
                    lpars = list(col = "blue", lwd = 3, lty = 1))   
     )
abline(v = mean(subset(tg, as.numeric(supp) == 2)$dose), col = "yellow1")

for (j in c("OJ","VC")) {
    for (i in c("05","10","20")) 
        assign(paste0(j,i), subset(tg, (dose == as.numeric(i)/10 & supp == j),len)$len)
}
```

&nbsp;

Though the efficacy of the supplements has seen positive result in promoting tooth growth for both supplement type as exhibited in the scatter plots in Figure-8, red curve on the left and blue curve on the right depicting the average growth rate for **OJ** supplement and **VC** supplement respectively. Note the average growth rate for **OJ** is more rapid at lower than mean dosage of `r mean(tg$dose)` doses (yellow vertical line) and tapering or slowing visibly at above the mean dosage, while average growth rate for **VC** is more uniform across the recommended dosage but tapering slightly or invisibly at above the mean dosage. Refer to the table-5 below, overall effectiveness of **OJ** is better in encouraging tooth growth at lower dosage (0.5 ~ 1 dose) compare to **VC** which is more potent at higher dosage (2 doses).  
&nbsp;

$Table-5$  

| Supplement x Dosage | Tooth Length (Mean, Min, Max) |  
|------------------------|---------------------------------------------------|  
| **OJ** x $\frac{1}{2}$ | `r mean(OJ05)`, `r range(OJ05)` |  
| **VC** x $\frac{1}{2}$ | `r mean(VC05)`, `r range(VC05)` |  
| **OJ** x 1 | `r mean(OJ10)`, `r range(OJ10)` |  
| **VC** x 1 | `r mean(VC10)`, `r range(VC10)` |  
| **OJ** x 2 | `r mean(OJ20)`, `r range(OJ20)` |  
| **VC** x 2 | `r mean(VC20)`, `r range(VC20)` |  
&nbsp;

Generally, we know that both supplements are effective for tooth growth, albeit varied rate at differing dosage. Refer to table-6 beneath, the average length of tooth growth attained for each supplement type vs theoretical mean listed, **OJ** type spurs longer average tooth length than the hypothetical mean $\mu$, than **VC** type which tooth length fall short of hypothetical mean $\mu$.  

Though both supplement types, **OJ** and **VC**, have seen positive growth rate, their average tooth length appeared to differ in which **OJ's** average tooth length is longer than hypothetical mean $\mu$ whereas **VC's** average tooth length is shorter than the $\mu$, seemed to suggest **OJ** is more potent than **VC**.  
&nbsp;

$Table-6$  

| Supplement | Average tooth length vs hypothetical mean $\mu$ `r round(mean(tg$len),4)` |  
|------------|-------------------------------|  
| **OJ** | `r round(mean(subset(tg,as.numeric(supp)==1)$len),4)` |  
| **VC** | `r round(mean(subset(tg,as.numeric(supp)==2)$len),4)` |  
&nbsp;

Thus far, we have relied on empirical evidence on our findings about the efficacy of the supplements on growth rate on tooth length, therefore a hypothesis test is mandatory to establish statistical evidence of 95% confidence level to substantiate our claims.  
&nbsp;

### Bootstrapping ToothGrowth sampling dataset  
&nbsp;

R's ToothGrowth sample dataset used in this course work is inherently not Gaussian in its distribution, so a bootstrapping procedure is needed to help it conform to the central limit theorem definition for subsequent statistical analysis. We will bootstrap ToothGrowth sample dataset to 2000 sampling data samples.  

Take note that ToothGrowth is a data table of 60 rows by 3 columns, the bootstrapping procedure will sample with replacement the 60 rows by rowid for each bootstrap sample, thus we will get different permutation of rows for each data sample in the bootstrapped dataset without altering the underlying intrinsic data definition.  
&nbsp;

```{r boot.strap, echo=TRUE}
#
# bootstrapping ToothGrowth sampling dataset
B.tg <- bootstrap(tg, B*2)

# calculating means for bootstrapped ToothGrowth sampling distribution
B.tg.len <- NULL
for (i in as.numeric(B.tg$.id)) {
    B.tg.len <- c(B.tg.len, mean(tg[B.tg$strap[[i]][[2]],]$len))
}

# Statistics estimates for ToothGrowth sampling mean distribution
B <- NROW(B.tg.len)
Xmu <- mean(B.tg.len)
Xsd <- sd(B.tg.len)
Xvar <- var(B.tg.len)
Xse <- sd(B.tg.len)/sqrt(B)

# Mean estimate for tooth length on OJ supplement
tg.oj <- NULL
XOJmu <- NULL
for (i in as.numeric(B.tg$.id)) {
    tg.oj <- subset(tg[B.tg$strap[[i]][[2]],], as.numeric(supp) == 1, len)
    XOJmu <- c(XOJmu, mean(tg.oj$len))
}
XOJmu <- mean(XOJmu)

# Mean estimate for tooth length on VC supplement
tg.oj <- NULL
XVCmu <- NULL
for (i in as.numeric(B.tg$.id)) {
    tg.oj <- subset(tg[B.tg$strap[[i]][[2]],], as.numeric(supp) == 2, len)
    XVCmu <- c(XVCmu, mean(tg.oj$len))
}
XVCmu <- mean(XVCmu)
```
&nbsp;

Let's take a look at the bootstrapped ToothGrowth sampling mean distribution $\bar{X}$ (Figure-9 & Figure-10), as oppose to its primal distribution (Figure-5 & Figure-6). It looks more Gaussian now than before.  
&nbsp;

```{r boot.strap.2}
b <- ggplot()
b <- b + geom_boxplot(data = as.data.frame(B.tg.len), mapping = aes(x = B.tg.len, colour = "Bootstrapped"), 
                      outlier.colour = "darkorange",
                      size = 0.5,
                      varwidth = FALSE)
b <- b + labs(x = "Tooth Length", caption = "Figure-9")
print(b)  
```

&nbsp;

```{r boot.strap.3}
h <- ggplot(data = as.data.frame(B.tg.len), mapping = aes(x = B.tg.len, y = ..density..)) 
h <- h + geom_histogram(color = "black", fill = "white", bins = 100) +
         geom_density(alpha = 0.5, fill = "#FF6666", col = "red") +
         geom_vline(aes(xintercept = mean(x = B.tg.len), colour = "Mean Length"), lty = 1, lwd = 1) +
        labs(x = "Tooth Length", caption = "Figure-10")
h <- h + scale_color_manual(name = "statistics", values = "blue")
print(h)
```

&nbsp;

$Table-7$  

| Statistics Estimates for $\bar{X}$ |  
|----------------------------|--------------------------------------------------|  
| $\mu_\bar{X} =$ `r round(Xmu,4)` | Mean of $\bar{X}$ |  
| $\sigma_\bar{X} =$ `r round(Xsd,4)` | Standard deviation of $\bar{X}$ |  
| $\sigma_\bar{X}^2 =$ `r round(Xvar,4)` | Variance of $\bar{X}$ |  
| $SE_\bar{X} =$ `r round(Xse,4)` | Standard Error of $\bar{X}$ |  
&nbsp;

Comparison between bootstrapped sampling mean and sample average tooth length by supplement type.  

$Table-8$  

| Supplement | Bootstrapped sampling mean vs Average tooth length |  
|------------|----------------------------------------------------|  
| **OJ** | `r round(XOJmu,4)` $vs$ `r round(mean(subset(tg, as.numeric(supp) == 1)$len),4)` |  
| **VC** | `r round(XVCmu,4)` $vs$ `r round(mean(subset(tg, as.numeric(supp) == 1)$len),4)` |  
&nbsp;
Take note the bootstrapped sampling means for both supplement types are equivalent to their respective average tooth length, which is inline with $\mu_\bar{X} = \mu$.
We may draw an initial conjecture that ToothGrowth sample dataset is a valid representation of the population.  
&nbsp;

### Hypothesis Test  
&nbsp;

We conduct a student's T-test to verify our hypothetical claim: supplements will encourage tooth growth.  
Hypothesis test set up as follow:  

**H~0~** : $\mu_\bar{X} = \mu$ $\{$Efficacy of the supplements is TRUE$\}$  

**H~A~** : $\mu_\bar{X} \neq \mu$ $\{$Efficacy of the supplements is NOT TRUE$\}$  
&nbsp;

```{r t.test.tg, echo=TRUE}
# t-test on bootstrapped dataset

# Confidence interval
CI_t <- Xmu + c(-1,1) * qt(1 - alpha/2, NROW(B.tg.len) - 1) * sqrt(Xvar/B)

# t-value 
t_val <- (Xmu - mean(tg$len))/Xse

# p-value
# p-value (as-is) for two-sided test
# p-value (x 2) for one-sided test
p_val <- pt(q = abs(t_val), df = B - 1, lower.tail = FALSE) * 2  #one-sided p-value
```
&nbsp;

$CI_t = \bar{X_\mu} \pm t_{(1-\frac{\alpha}{2},B-1)}\sqrt\frac{\sigma_\bar{X}^2}{B} =$ `r round(CI_t, 4)`  

$t.value = \frac{\bar{X_\mu}-\mu}{SE_\bar{X}} =$ `r round(t_val, 4)`  

$p.value = pt(q=abs(t.value),df=B-1,lower.tail=F)*2 =$ `r round(p_val, 4)`  
&nbsp;

$p.value =$ **`r round(p_val, 4)`** $>$ $\alpha$ $5\%$ level implying our hypothesis test failed to reject the **H~0~**, that is to say **H~0~** is accepted for our hypothetical claim that both supplement types are effective for tooth growth with 95% confidence. In addition, the confidence interval (`r round(CI_t, 4)`) from the test seem to be hugging closely around the theoretical mean $\mu$ `r round(mean(tg$len),4)` augmenting the assumption under **H~0~** is true, which mean the theoretical sample mean $\mu$ is true mean of the population. By virtue of this conclusion, we can deduce that our earlier conjecture about **OJ** is more potent than **VC** is also true.  
&nbsp;
&nbsp;

