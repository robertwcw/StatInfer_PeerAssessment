---
title: "Statistical Inference Course Project"
author: "robertwcw"
date: "12/13/2020"
output: 
  html_document: 
    fig_caption: yes
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
# Sys.setenv(TZ = "UTC")      # set global TZ to UTC for POSIXt class

# .Rfliburl <- "https://raw.githubusercontent.com/robertwcw/Rflib/master"
# source(file.path(.Rfliburl,"getRflib.R"),local = TRUE)
# source(getRflib("is.defined.R"),local = TRUE)
# source(getRflib("myplclust.R"),local = TRUE)
# source(getRflib("strCap.R"),local = TRUE)
```

```{r init.local}
if (!requireNamespace("ggplot2",quietly = TRUE)) install.packages("ggplot2")
library(ggplot2)

if (!requireNamespace("datasets",quietly = TRUE)) install.packages("datasets")
library(datasets)

if (!requireNamespace("modelr",quietly = TRUE)) install.packages("modelr")
library(modelr)

par.def <- par(no.readonly = TRUE) 
```
&nbsp;

# Synopsis

This course work embarks on the objective to help us understand how simulation and Central Limit Theorem (CLT) are useful in statistical inference studies (resampling, confidence interval and hypothesis testing etc...) when real-world data isn't readily available or an ideal size of sampling data is constrained by the inevitable factors such as budget, time-limit and manpower etc. Simulation is handy for pumping up the sampling data size to enable a more normal distribution approach for statistical analysis, while CLT states that a sampling distribution of the statistic estimates for a variable, of sufficiently large sample size will approximate normal distribution regardless of the variable's distribution in the population, with the sampling estimates converging around the population estimates.

First part of the course work is to gauge our understanding of Statistical Simulation and Central Limit Theorem.  

Second part of the course work is to assess our comprehension in basic Statistical Inference using the R's "ToothGrowth" dataset.  
&nbsp;

## Statistical Simulation

This course work will require two simulated random exponential distribution datasets, namely $X_e$ (X-exponential) and $X_\mu$ (X-mu), to demonstrate how CLT explains the statistics of a sample dataset regardless of its distribution in the population from where the sample was drawn, will eventually fall into a normal distribution pattern when the size of sampling data increased to sufficient number.

Specification of the two datasets used in this demonstration, as follows:  

|   |   |  
|------------------|------------------------------------------------------------|  
| $n = 40$ | Number of random exponential per sample |  
| $B = 1000$ | Number of simulation |  
| $\lambda = \frac {1} {5}$ | Rate value for random exponential function |  
| $\mu = \frac {1} {\lambda}$ | Theoretical mean |  
| $\sigma  = \frac {1} {\lambda}$ | Theoretical standard deviation |  
| $X_e$ | Simulated dataset of 1000 random exponential |  
| $X_\mu$ | Simulated dataset of 1000 sample means of 40 random exponential per sample |
&nbsp;

```{r stat.simul, echo=TRUE}
# statistical simulation

n <- 40                 #number of exponential per sample 
B <- 1000               #number of simulation 
lambda <- 1/5           #rate value for random exponential function
mu <- 1/lambda          #theoretical mean of population
sigma <- 1/lambda       #theoretical std-deviation of population
alpha <- 1/20           #alpha level for confidence interval test

# generate 1000 simulated distribution of 40 random exponential per sample
z <- matrix(nrow = B, ncol = n) 
for (i in 1:B) {z[i,] <- rexp(n, lambda)}; rm(i)

e <- NULL
e <- cbind(rexp(n*25, lambda), # 1000 sample random exponential distribution data
           apply(z, 1, mean))  # 1000 simulated means of each sample of 40 random exponential distributed data
e <- as.data.frame(e)
colnames(e) <- c("X_e", "X_mu")
```
&nbsp;

| Statistics Estimates of $X_\mu$ |
|------------------------|---------------------------------------------------|
| $\bar{X_\mu}$ = `r round(mean(e$X_mu), 4)` | Mean of $X_\mu$ |  
| $\sigma_{X_\mu}$ = `r round(sd(e$X_mu), 4)` | Standard Deviation of $X_\mu$ |  
| $\sigma_{X_\mu}^2$ = `r round(var(e$X_mu), 4)` | Variance of $X_\mu$ |  
| $SE_{X_\mu}$ = `r round(sd(e$X_mu)/sqrt(B), 4)` | Standard Error of $X_\mu$ |  
&nbsp;

5-Number Summary of $X_\mu$  
```{r Xmu.summa}
summary(e$X_mu)
```
&nbsp;

```{r plot.Xmu.boxplot}
# boxplot for bootstrapped mean of random exponential sample data 'X_mu'
b <- ggplot()
b <- b + geom_boxplot(data = e, mapping = aes(x = X_mu), 
                      outlier.colour = "darkorange",
                      size = 0.5,
                      varwidth = FALSE)
b <- b + labs(x = expression("X"[mu]), caption = "Figure-1")
print(b)  
```
&nbsp;

The shape formation of boxplot graph for dataset $X_\mu$ in Figure-1 suggests that it is normal distributed. Construct of the boxplot spot the median occurs near the center inside the box with the lower and upper whiskers just about equal length on both sides of the box shall assert our premise about the distribution of $X_\mu$ is normal. Juxtaposing the boxplot in Figure-1 and the histogram in Figure-2 further strengthen our assertion.  
&nbsp;

```{r plot.Xmu.hist}
# histogram for bootstrapped of 1000 mean of random exponential sample data 'X_mu'
h <- ggplot(data = e, mapping = aes(x = X_mu, y = ..density..)) 
h <- h + geom_histogram(color = "black", fill = "white", bins = n) +
        geom_density(alpha = 0.5, fill = "#FF6666", col = "red") +
        geom_vline(aes(xintercept = mean(x = X_mu)), lty = 1, lwd = 1, col = "blue") +
        geom_vline(aes(xintercept = mu), lty = 2, lwd = 0.8, col = "green") +
        labs(x = expression("X"[mu]), caption = "Figure-2")
print(h)
```
&nbsp;

In Figure-2, the histogram graph exhibits a symmetrical (almost) bell-shape density curve overlapping on the histogram suggesting the $X_\mu$ dataset approximates normal distribution. Mean $\bar{X_\mu}$ = `r round(mean(e$X_mu), 4)` is said to be statistically equal to theoretical mean $\mu$, implying that the sample mean is the true mean of the population and this assumption is reinforced by the sample variance $\sigma_{X_\mu}^2 =$ `r round(var(e$X_mu), 4)` and sample standard error $SE_{X_\mu} =$ `r round(sd(e$X_mu)/sqrt(B), 4)` respectively signify the data points of $X_\mu$ will converge around the theoretical mean. 

While both theoretical variance $\sigma^2$ and sample variance $\sigma_{X_\mu}^2$ are used for measuring the average distance the data points from the mean of respective population and sample dataset. Compare $\sigma^2 = 25$ against $\sigma_{X_\mu}^2 =$ `r round(var(e$X_mu), 4)` is quite apparent to our naked eyes that the sample variance is more representative of the variability of the population data than the theoretical variance if the sample mean $\bar{X_\mu}$ is true and equivalent to the theoretical population mean.  
&nbsp;

```{r h.test, echo=TRUE}
# t-test
CI_t <- mean(e$X_mu) + c(-1,1) * qt(1 - alpha/2, NROW(e$X_mu) - 1) * sqrt(var(e$X_mu)/length(e$X_mu))
t_val <- (mean(e$X_mu) - mu)/(sd(e$X_mu)/sqrt(NROW(e$X_mu)))
p_val <- pt(q = abs(t_val), df = B - 1, lower.tail = FALSE) * 2  #one-sided p-value
```
&nbsp;
<!-- # z-test -->
<!-- CI_z <- mean(e$X_mu) + c(-1,1) * qnorm(1 - alpha/2) * sqrt(var(e$X_mu)/B) -->
<!-- z_val <- (mean(e$X_mu) - mu)/(sd(e$X_mu)/sqrt(NROW(e$X_mu))) -->
<!-- p_val <- pnorm(abs(z_val), lower.tail = FALSE) * 2  #p-value for one-sided test -->
<!-- # p_val <- pnorm(abs(z_val), lower.tail = FALSE)      #p-value for two-sided test -->

Let's do a student's T-test to verify our claims; hypothesis test set up as follow:  
**H~0~** : $\bar{X_\mu} = \mu$  

**H~A~** : $\bar{X_\mu} \neq \mu$  

$CI_t = \bar{X_\mu} \pm t_{(1-\frac{\alpha}{2},B-1)}\sqrt\frac{\sigma_\bar{X}^2}{B} =$ `r round(CI_t, 4)`  

$t.value = \frac{\bar{X_\mu}-\mu}{SE_\bar{X_\mu}} =$ `r round(t_val, 4)`  

$p.value = pt(q=abs(t.value),df=B-1,lower.tail=F)*2 =$ `r round(p_val, 4)`  

$p.value =$ `r round(p_val, 4)` $> \alpha$ `r alpha*100`$\%$ level implies the hypothesis test failed to reject true **H~0~** , which is saying the true **H~0~** is accepted for our assumption was true that the sample mean $\bar{X_\mu}$ postulating the true mean of the population. Confidence interval $CI_t$ (lower = `r round(CI_t[1], 4)` & upper = `r round(CI_t[2], 4)`) tightly clasping around the theoretical mean $\mu$ indicates the sample estimates will occur very closely to the $\bar{X_\mu}$ between lower bound and upper bound $95\%$ of the time. In other words, the sample mean $\bar{X_\mu}$ is estimating what it was intended to measure. On the same note our claim that the sample variance was true for the population is correct.

### Sample Mean vs Theoretical Mean

Results from statistical simulation tell us that the sample mean $\bar{X_\mu} =$ `r round(mean(e$X_mu), 4)` is estimating the population's theoretical mean $\mu$. 

### Sample Variance vs Theoretical Variance

Sample variance $\sigma_\mu^2 =$ `r round(var(e$X_mu), 4)` is much smaller than the population's theoretical variance $\sigma^2$, hinting the population distribution is not Gaussian as compare to the sample distribution.  
&nbsp;

Note that the vertical blue solid line plotted on the histogram graph in Figure-2 represents the sample mean $\bar{X_\mu}$ which is positioned ever so closely next to the theoretical mean $\mu$, where $t.value$ will tells us if $\bar{X_\mu}$ is less than or greater than $\mu$.  
&nbsp;

### Distributions

Though Central Limit Theorem (CLT) states that the distribution of the sample statistic will become more Gaussian when the number of sampling data is sufficiently large. Let us take the dataset $X_e$ consisting of 1000 random exponential, which we believe is sizeable enough a sample to demonstrate how certain it will look Gaussian according to the CLT definition. 
&nbsp;

| Statistics Estimates of $X_e$ | |
|------------------------|---------------------------------------------------|
| $\bar{X_e}$ = `r round(mean(e$X_e), 4)` | Mean of $X_e$ |  
| $\sigma_{X_e}$ = `r round(sd(e$X_e), 4)` | Standard Deviation of $X_e$ |  
| $\sigma_{X_e}^2$ = `r round(var(e$X_e), 4)` | Variance of $X_e$ |  
| $SE_{X_e}$ = `r round(sd(e$X_e)/sqrt(B), 4)` | Standard Error of $X_e$ |  
&nbsp;

5-Number Summary of $X_e$     
```{r Xe.summa}
summary(e$X_e)
```
&nbsp;

```{r plot.Xe.boxplot}
# boxplot for exponential distributed random sample data 'X_e'
b <- ggplot()
b <- b + geom_boxplot(data = e, mapping = aes(x = X_e), 
                      outlier.colour = "darkorange",
                      size = 0.5,
                      varwidth = FALSE)
b <- b + labs(x = expression("X"[e]), caption = "Figure-3")
print(b)
```
&nbsp;

```{r plot.Xe.hist}
# histogram for 1000 random exponential sample data 'X_e'
h <- ggplot(data = e, mapping = aes(x = X_e, y = ..density..)) 
h <- h + 
    geom_histogram(color = "black", fill = "white", bins = n) +
    geom_density(alpha = 0.5, fill = "#FF6666", col = "red") +
    geom_vline(aes(xintercept = mean(x = X_e)), lty = 1, lwd = 1, col = "blue") +
    geom_vline(aes(xintercept = mu), lty = 2, lwd = 0.8, col = "green") +
    labs(x = expression("X"[e]), caption = "Figure-4")
print(h)

```
&nbsp;

Boxplot graph in Figure-3 and histogram (with density curve) graph in Figure-4 are plotted based on the same $X_e$ dataset, clearly they don't look Gaussian at all but are skewed (right skewed in this case). The phenomenon explains that CLT only works on the sampling statistic e.g. sampling means, but not on the as-is sampling data irregardless of size.

Note that mean $\bar{X_e}$ and variane $\sigma_{X_e}^2$ of the dataset $X_e$ are both echoing the theoretical mean $\mu$ and $\sigma$, whereas only the $\bar{X_\mu}$ mimic the theoretical mean but $\sigma_{X_\mu}^2$ does not resonate theoretical variance, which is in line with our prior supposition about the population distribution is not Gaussian compare to the sample distribution.  
&nbsp;


## Basic Inferential Data Analysis  

This is the second part of this course work. We will perform basic statistical inferential data analysis on the coefficient between tooth growth and supplement types and dosage. Include herein the structure of ToothGrowth dataset loaded into RStudio's work space.  
```{r load.ToothGrowth}
data("ToothGrowth")
delayedAssign("tg", ToothGrowth)
rm(ToothGrowth)
```
&nbsp;

### Exploratory data analysis  

ToothGrowth sampling dataset has 60 rows with 3 variables stored in data frame structure.  
- len ......: Length of tooth  
- supp ...: supplement type {OJ $= 1$, VC $= 2$}  
- dose ...: dosage {$\frac{1}{2}$, $1$ or $2$}  
&nbsp;

#### Data Structure  
```{r eda.ToothGrowth.1}
# reveal the data structure of ToothGrowth
str(tg)
```

&nbsp;

#### 5-Number summary  
```{r eda.ToothGrowth.2}
summary(tg)
```
&nbsp;

#### Theoretical Statistics  
Let's assume the following theoretical statistics for the population data that we are attempting to infer. Theoretical statistics are generally referring to the population statistics that are unknown at this stage, yet to be ascertained via hypothesis test.  

| Theoretical Statistics for the population | |
|------------------------|---------------------------------------------------|
| $\mu$ = `r round(mean(tg$len),4)` | Theoretical mean |  
| $\sigma$ = `r round(sd(tg$len),4)` | Theoretical standard deviation |  
| $\sigma^2$ = `r round(var(tg$len),4)` | Theoretical variance |  
&nbsp;

#### Exploratory graph plots  
Figure-5 and Figure-6 are initial exploratory plots on ToothGrowth sampling data (see Data Structure). The data distribution does not look Gaussian.  

```{r eda.ToothGrowth.3}
b <- ggplot()
b <- b + geom_boxplot(data = tg, mapping = aes(x = len), 
                      outlier.colour = "darkorange",
                      size = 0.5,
                      varwidth = FALSE)
b <- b + labs(x = "Tooth Length", caption = "Figure-5")
print(b)  
```

&nbsp;

```{r eda.ToothGrowth.4}
h <- ggplot(data = tg, mapping = aes(x = len, y = ..density..)) 
h <- h + geom_histogram(color = "black", fill = "white", bins = 60) +
         geom_density(alpha = 0.5, fill = "#FF6666", col = "red") +
         geom_vline(aes(xintercept = mean(x = len)), lty = 1, lwd = 1, col = "blue") +
        labs(x = "Tooth Length", caption = "Figure-6")
print(h)

```

&nbsp;

#### Data analysis summary  
```{r eda.ToothGrowth.5, warning=FALSE, fig.cap="Figure-7", fig.align="left"}
scatter.smooth(x = tg$dose, y = tg$len,
               ylab = "Tooth Length",
               xlab = "Supplements Dosage",
               lpars = list(col = "green", lwd = 3, lty = 3))   
```
&nbsp;

There is an overall up trend in the average rate of tooth growth length on the test subjects administered with either **OJ** supplement or **VC** supplement. The test subjects were separated into two groups of 30 participants each by supplement type. In Figure-7, the green dotted curve drawn on the scatter plot exhibits a phenomenon where the test subjects given lower dosage of between 1/2 to 1 dose experienced faster average tooth growth rate than test subjects given higher dosage of 2 doses experienced slower average growth rate.  

Let's take the scatter plot further by breaking down the supplements administered on the test subjects into two groups by type **OJ** and type **VC**.
&nbsp;

```{r eda.ToothGrowth.6, warning=FALSE, fig.cap="Figure-8", fig.align="left"}
par(mfrow = c(1,2))

with(subset(tg, as.numeric(supp) == 1), 
     scatter.smooth(x = dose, y = len,
                    main = "OJ Supplement",
                    ylab = "Tooth Length",
                    xlab = "Dosage",
                    lpars = list(col = "red", lwd = 3, lty = 3))   
     )
abline(v = mean(subset(tg, as.numeric(supp) == 1)$dose), col = "yellow1")

with(subset(tg, as.numeric(supp) == 2), 
     scatter.smooth(x = dose, y = len,
                    main = "VC Supplement",
                    ylab = "Tooth Length",
                    xlab = "Dosage",
                    lpars = list(col = "blue", lwd = 3, lty = 3))   
     )
abline(v = mean(subset(tg, as.numeric(supp) == 2)$dose), col = "yellow1")

for (j in c("OJ","VC")) {
    for (i in c("05","10","20")) 
        assign(paste0(j,i), subset(tg, (dose == as.numeric(i)/10 & supp == j),len)$len)
}
```

&nbsp;

Though the efficacy of the supplements has seen positive result in promoting tooth growth for both **OJ** type and **VC** type as exhibited in the scatter plots in Figure-8, red curve on the left and blue curve on the right depicting the average growth rate for **OJ** supplement and **VC** supplement respectively. Note that average growth rate for **OJ** is more rapid at lower than mean dosage `r mean(tg$dose)` (yellow vertical line) and tapered at above mean dosage, while average growth rate for **VC** is more uniform across the recommended dosage. Referring to the table attached below, overall effectiveness of **OJ** is better in encouraging tooth growth at lower dosage (0.5 ~ 1 dose) compare to **VC** which is more potent at higher dosage (2 doses).  
&nbsp;

| Supplement x Dosage | Tooth Length (Mean, Min, Max) |  
|------------------------|---------------------------------------------------|  
| **OJ** x $\frac{1}{2}$ | `r mean(OJ05)`, `r range(OJ05)` |  
| **VC** x $\frac{1}{2}$ | `r mean(VC05)`, `r range(VC05)` |  
| **OJ** x 1 | `r mean(OJ10)`, `r range(OJ10)` |  
| **VC** x 1 | `r mean(VC10)`, `r range(VC10)` |  
| **OJ** x 2 | `r mean(OJ20)`, `r range(OJ20)` |  
| **VC** x 2 | `r mean(VC20)`, `r range(VC20)` |  
&nbsp;

Generally, we know that both supplements are effective for tooth growth, albeit varied rate at differing dosage. The average length of tooth growth attained for each supplement type vs theoretical mean listed herein, **OJ** type spurs longer average tooth length than the hypothetical mean $\mu$, than **VC** type which tooth length is fall short of hypothetical mean $\mu$.  

Thus far, we have relied on empirical evidence on our findings about the efficacy of the supplements on growth rate on tooth length, therefore a hypothesis test is mandatory to establish statistical evidence of 95% confidence level to substantiate our claims.  
&nbsp;

| Supplement | Average tooth length vs hypothetical mean $\mu$ `r round(mean(tg$len),4)` |  
|------------|-------------------------------|  
| **OJ** | `r round(mean(subset(tg,as.numeric(supp)==1)$len),4)` |  
| **VC** | `r round(mean(subset(tg,as.numeric(supp)==2)$len),4)` |  
&nbsp;
Though both supplement types, **OJ** and **VC**, have seen positive growth rate, their average tooth length appeared to differ in which **OJ's** greater than hypothetical mean $\mu$ whereas **VC's** lesser than the $\mu$, seemed to suggest **OJ** is more potent than **VC**.  
&nbsp;

### Bootstrapping ToothGrowth sampling dataset  
&nbsp;

R's ToothGrowth sampling dataset used in this course work is not Gaussian in its distribution, so a bootstrapping procedure will help it conform to the central limit theorem definition for subsequent statistical analysis. We will bootstrapped ToothGrowth dataset to 2000 sampling data samples. 

Take note that ToothGrowth is a data table of 60 rows by 3 columns, the bootstrapping procedure will sample with replacement the 60 rows by rowid for each bootstrap sample, thus we will get differing permutation of rows for each data sample in the bootstrapped dataset without altering the underlying intrinsic data definition.  
&nbsp;

```{r boot.strap, echo=TRUE}
#
# bootstrapping ToothGrowth sampling dataset
B.tg <- bootstrap(tg, B*2)

# calculating means for bootstrapped ToothGrowth sampling distribution
B.tg.len <- NULL
for (i in as.numeric(B.tg$.id)) {
    B.tg.len <- c(B.tg.len, mean(tg[B.tg$strap[[i]][[2]],]$len))
}

# Statistics estimates for ToothGrowth sampling mean distribution
B <- NROW(B.tg.len)
Xmu <- mean(B.tg.len)
Xsd <- sd(B.tg.len)
Xvar <- var(B.tg.len)
Xse <- sd(B.tg.len)/sqrt(B)

# Mean estimate for tooth length on OJ supplement
tg.oj <- NULL
XOJmu <- NULL
for (i in as.numeric(B.tg$.id)) {
    tg.oj <- subset(tg[B.tg$strap[[i]][[2]],], as.numeric(supp) == 1, len)
    XOJmu <- c(XOJmu, mean(tg.oj$len))
}
XOJmu <- mean(XOJmu)

# Mean estimate for tooth length on VC supplement
tg.oj <- NULL
XVCmu <- NULL
for (i in as.numeric(B.tg$.id)) {
    tg.oj <- subset(tg[B.tg$strap[[i]][[2]],], as.numeric(supp) == 2, len)
    XVCmu <- c(XVCmu, mean(tg.oj$len))
}
XVCmu <- mean(XVCmu)
```
&nbsp;

Let's take a look at the bootstrapped ToothGrowth sampling mean distribution $\bar{X}$ (Figure-9 & Figure-10), as oppose to its primal distribution (Figure-5 & Figure-6). It looks more Gaussian now than before.  
&nbsp;

```{r boot.strap.2}
b <- ggplot()
b <- b + geom_boxplot(data = as.data.frame(B.tg.len), mapping = aes(x = B.tg.len), 
                      outlier.colour = "darkorange",
                      size = 0.5,
                      varwidth = FALSE)
b <- b + labs(x = "Tooth Length", caption = "Figure-9")
print(b)  
```

&nbsp;

```{r boot.strap.3}
h <- ggplot(data = as.data.frame(B.tg.len), mapping = aes(x = B.tg.len, y = ..density..)) 
h <- h + geom_histogram(color = "black", fill = "white", bins = 100) +
         geom_density(alpha = 0.5, fill = "#FF6666", col = "red") +
         geom_vline(aes(xintercept = mean(x = B.tg.len)), lty = 1, lwd = 1, col = "blue") +
        labs(x = "Tooth Length", caption = "Figure-10")
print(h)
```

&nbsp;

| Statistics Estimates for $\bar{X}$ |
|----------------------------|--------------------------------------------------|
| $\mu_\bar{X} =$ `r round(Xmu,4)` | Mean of $\bar{X}$ |  
| $\sigma_\bar{X} =$ `r round(Xsd,4)` | Standard deviation of $\bar{X}$ |  
| $\sigma_\bar{X}^2 =$ `r round(Xvar,4)` | Variance of $\bar{X}$ |  
| $SE_\bar{X} =$ `r round(Xse,4)` | Standard Error of $\bar{X}$ |  
&nbsp;

Comparison between bootstrapped sampling mean against average tooth length by supplement type.  

| Supplement | Bootstrapped sampling mean vs Average tooth length |  
|------------|----------------------------------------------------|  
| **OJ** | `r round(XOJmu,4)` $vs$ `r round(mean(subset(tg, as.numeric(supp) == 1)$len),4)` |  
| **VC** | `r round(XVCmu,4)` $vs$ `r round(mean(subset(tg, as.numeric(supp) == 1)$len),4)` |  
&nbsp;
Take note the bootstrapped sampling means for both supplement types are equivalent to their respective average tooth length, implying the supplements may be effective to promote tooth growth.  

### Hypothesis Test  
&nbsp;

We conduct a student's T-test to verify our hypothetical claim: supplements will encourage tooth growth. Hypothesis test set up as follow:  

**H~0~** : $\mu_\bar{X} = \mu$ $\{$Efficacy of the supplements is TRUE$\}$  

**H~A~** : $\mu_\bar{X} \neq \mu$ $\{$Efficacy of the supplements is NOT TRUE$\}$  
&nbsp;

```{r t.test.tg, echo=TRUE}
#
# t-test for ToothGrowth dataset

# Confidence interval
CI_t <- Xmu + c(-1,1) * qt(1 - alpha/2, NROW(B.tg.len) - 1) * sqrt(Xvar/B)

# t-value 
t_val <- (Xmu - mean(tg$len))/Xse

# p-value
p_val <- pt(q = abs(t_val), df = B - 1, lower.tail = FALSE) * 2  #one-sided p-value
```
&nbsp;

$CI_t = \bar{X_\mu} \pm t_{(1-\frac{\alpha}{2},B-1)}\sqrt\frac{\sigma_\bar{X}^2}{B} =$ `r round(CI_t, 4)`  

$t.value = \frac{\bar{X_\mu}-\mu}{SE_\bar{X}} =$ `r round(t_val, 4)`  

$p.value = pt(q=abs(t.value),df=B-1,lower.tail=F)*2 =$ `r round(p_val, 4)`  
&nbsp;

$p.value =$ **`r round(p_val, 4)`** $>$ $\alpha$ $5\%$ level implying our hypothesis test failed to reject the **H~0~**, that is to say **H~0~** is accepted for our hypothetical claim that both supplement types are effective for tooth growth with 95% confidence. In addition, the confidence interval (`r round(CI_t, 4)`) from the test seem to be hugging closely around the theoretical mean $\mu$ `r round(mean(tg$len),4)` augmenting the assumption under **H~0~** is true, which mean the theoretical sample mean $\mu$ is true mean of the population. By virtue of this conclusion, we can deduce that our earlier conjecture about **OJ** is more potent than **VC** is also true.  
&nbsp;


